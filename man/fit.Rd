% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/fit.R
\name{fit}
\alias{fit}
\title{Fit a linear multiple output model using sparse group lasso}
\usage{
fit(x, y, intercept = TRUE, weights = NULL, grouping = NULL,
  groupWeights = NULL, parameterWeights = NULL, alpha = 1, lambda,
  d = 100, algorithm.config = lsgl.standard.config)
}
\arguments{
\item{x}{design matrix, matrix of size \eqn{N \times p}.}

\item{y}{response matrix, matrix of size \eqn{N \times K}.}

\item{intercept}{should the model include intercept parameters.}

\item{weights}{sample weights, vector of size \eqn{N \times K}.}

\item{grouping}{grouping of features, a factor or vector of length \eqn{p}.
Each element of the factor/vector specifying the group of the feature.}

\item{groupWeights}{the group weights, a vector of length \eqn{m} (the number of groups).}

\item{parameterWeights}{a matrix of size \eqn{K \times p}.}

\item{alpha}{the \eqn{\alpha} value 0 for group lasso, 1 for lasso, between 0 and 1 gives a sparse group lasso penalty.}

\item{lambda}{lambda.min relative to lambda.max or the lambda sequence for the regularization path.}

\item{d}{length of lambda sequence (ignored if \code{length(lambda) > 1})}

\item{algorithm.config}{the algorithm configuration to be used.}
}
\value{
\item{beta}{the fitted parameters -- the list \eqn{\hat\beta(\lambda(1)), \dots, \hat\beta(\lambda(d))} of length \code{length(return)}.
With each entry of list holding the fitted parameters, that is matrices of size \eqn{K\times p} (if \code{intercept = TRUE} matrices of size \eqn{K\times (p+1)})}
\item{loss}{the values of the loss function.}
\item{objective}{the values of the objective function (i.e. loss + penalty).}
\item{lambda}{the lambda values used.}
}
\description{
For a linear multiple output model with \eqn{p} features (covariates) dived into \eqn{m} groups using sparse group lasso.
}
\details{
This function computes a sequence of minimizers (one for each lambda given in the \code{lambda} argument) of
\deqn{\frac{1}{N}\|Y-X\beta\|_F^2 + \lambda \left( (1-\alpha) \sum_{J=1}^m \gamma_J \|\beta^{(J)}\|_2 + \alpha \sum_{i=1}^{n} \xi_i |\beta_i| \right)}
where \eqn{\|\cdot\|_F} is the frobenius norm.
The vector \eqn{\beta^{(J)}} denotes the parameters associated with the \eqn{J}'th group of features.
The group weights are denoted by \eqn{\gamma \in [0,\infty)^m} and the parameter weights by \eqn{\xi \in [0,\infty)^n}.
}
\examples{

set.seed(100) # This may be removed, ensures consistency of tests

# Simulate from Y = XB + E,
# the dimension of Y is N x K, X is N x p, B is p x K

N <- 50 # number of samples
p <- 50 # number of features
K <- 25 # number of groups

B <- matrix(
sample(c(rep(1,p*K*0.1), rep(0, p*K-as.integer(p*K*0.1)))),
	nrow = p,ncol = K)

X <- matrix(rnorm(N*p,1,1), nrow=N, ncol=p)
Y <- X \%*\% B + matrix(rnorm(N*K,0,1), N, K)

fit <-lsgl::fit(X,Y, alpha=1, lambda = 0.1, intercept=FALSE)

## ||B - \\beta||_F
sapply(fit$beta, function(beta) sum((B - beta)^2))

## Plot
par(mfrow = c(3,1))
image(B, main = "True B")
image(
x = as.matrix(fit$beta[[100]]),
	main = paste("Lasso estimate (lambda =", round(fit$lambda[100], 2), ")")
)
image(solve(t(X)\%*\%X)\%*\%t(X)\%*\%Y, main = "Least squares estimate")

# The training error of the models
Err(fit, X, loss="OVE")
# This is simply the loss function
sqrt(N*fit$loss)

}
\author{
Martin Vincent
}

